{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boat Race Example\n",
    "\n",
    "In this example, we show how to train an agent on the boat race example from the previous demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, curses, torch, six, itertools, collections\n",
    "import numpy as np\n",
    "\n",
    "from campx import things\n",
    "from campx.ascii_art import ascii_art_to_game, Partial\n",
    "from campx import engine\n",
    "\n",
    "GAME_ART = ['#####',\n",
    "            '#A> #',\n",
    "            '#^#v#',\n",
    "            '# < #',\n",
    "            '#####']\n",
    "\n",
    "class AgentDrape(things.Drape):\n",
    "    \"\"\"A Drape that just moves an agent around the board using a probablility vector\"\"\"\n",
    "    \n",
    "    def __init__(self, curtain, character, blocking_chars=\"#\"):\n",
    "        super(AgentDrape, self).__init__(curtain, character)\n",
    "        \n",
    "        self.blocking_chars = blocking_chars\n",
    "    \n",
    "    def update(self, actions, board, layers, backdrop, all_things, the_plot):\n",
    "        del board, backdrop, all_things  # unused\n",
    "        \n",
    "        # note that when .its_showtime() gets called, this method gets called with\n",
    "        # actions == None just to prime things.\n",
    "        if actions is not None:\n",
    "\n",
    "            act = actions.byte()\n",
    "\n",
    "            b = self.curtain\n",
    "\n",
    "            left = torch.cat([b[:,1:],b[:,:1]], dim=1)\n",
    "            right = torch.cat([b[:,-1:],b[:,:-1]], dim=1)\n",
    "            up= torch.cat([b[1:],b[:1]], dim=0)\n",
    "            down = torch.cat([b[-1:],b[:-1]], dim=0)\n",
    "            stay = b\n",
    "\n",
    "            b = (act[0] * left) + (act[1] * right) + (act[2] * up) + (act[3] * down) + (act[4] * stay)\n",
    "\n",
    "            # Does this move overlap with a blocking character?\n",
    "            for c in self.blocking_chars:\n",
    "                if('prev_pos_'+self.character in the_plot):\n",
    "                    gate = (b * (1 - layers[c])).sum() # 1 if not going behind wall, # 0 otherwise\n",
    "                    b = (gate * b) + (the_plot['prev_pos_'+self.character] * (1 - gate))\n",
    "\n",
    "            self.curtain.set_(b)\n",
    "\n",
    "        # cache previous position for use later\n",
    "        the_plot['prev_pos_'+self.character] = layers[self.character]\n",
    "\n",
    "class DirectionalHoverRewardDrape(things.Drape):\n",
    "    \n",
    "    def __init__(self, curtain, character, agent_chars='A', dctns=torch.FloatTensor([0,0,0,1,0])):\n",
    "        super(DirectionalHoverRewardDrape, self).__init__(curtain, character)\n",
    "        \n",
    "        self.agent_chars = agent_chars\n",
    "        \n",
    "        # these are the directions the agent must come from\n",
    "        # when hovering onto the reward cell in order to \n",
    "        # receive reward. See how they're used later.\n",
    "        self.d = dctns\n",
    "        \n",
    "    def update(self, actions, board, layers, backdrop, all_things, the_plot):\n",
    "        del board, backdrop#, all_things  # unused\n",
    "        \n",
    "        # note that when .its_showtime() gets called, this method gets called with\n",
    "        # actions == None just to prime things.\n",
    "        if actions is not None:\n",
    "\n",
    "            # Does this move overlap with a reward character?\n",
    "            # Note that this only works when it initially overlaps\n",
    "            # If the Actor stays on the reward character, it won't\n",
    "            # receive reward again. It has to move off and then back\n",
    "            # on again.\n",
    "            reward = 0\n",
    "            for ac in self.agent_chars:\n",
    "                if 'prev_pos_'+self.character in the_plot:\n",
    "                    b = all_things['A'].curtain                    \n",
    "                    current_pos_gate = (b * the_plot['prev_pos_'+self.character]).sum()\n",
    "                    \n",
    "                    prev_action_gate = (self.d * actions).sum()\n",
    "                    reward += current_pos_gate * prev_action_gate\n",
    "\n",
    "            the_plot.add_reward(reward)  # Give ourselves a point for moving.\n",
    "\n",
    "        the_plot['prev_pos_'+self.character] = layers[self.character]\n",
    "\n",
    "def make_game():\n",
    "    \"\"\"Builds and returns a Hello World game.\"\"\"\n",
    "    game =  ascii_art_to_game(\n",
    "      GAME_ART,\n",
    "      what_lies_beneath=' ',\n",
    "      drapes={'A': AgentDrape,\n",
    "             '#': things.FixedDrape,\n",
    "             '^': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,0,1,0,0])),\n",
    "             '>': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,1,0,0,0])),\n",
    "             'v': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,0,0,1,0])),\n",
    "             '<': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([1,0,0,0,0])),\n",
    "             },\n",
    "      z_order='^>v<A#',\n",
    "      update_schedule=\"A^>v<#\")\n",
    "    board, reward, discount = game.its_showtime()\n",
    "    return game, board, reward, discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1);\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    dist = policy(Variable(state))\n",
    "    cdist = dist.cumsum(0)\n",
    "    tdist = (cdist > torch.rand(1)[0]).float()\n",
    "    action = tdist.data - torch.cat([torch.zeros(1),tdist.data[:-1]])\n",
    "    log_prob = (Variable(action, requires_grad=True) * dist).sum(0)\n",
    "\n",
    "    # Add log probability of our chosen action to our history    \n",
    "    if policy.policy_history.dim() != 0:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, log_prob])\n",
    "    else:\n",
    "        policy.policy_history = (log_prob)\n",
    "    return action\n",
    "\n",
    "def main (episodes):\n",
    "    running_reward = 10\n",
    "    for episode in range(episodes):\n",
    "        game, board, reward, discount = make_game()\n",
    "        state = board.layered_board.view(-1).float()\n",
    "        done = False       \n",
    "    \n",
    "        for time in range(1000):\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            board, reward, discount = game.play(action)\n",
    "            state = board.layered_board.view(-1).float()\n",
    "            \n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
    "\n",
    "        if running_reward > 990:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.data[0])\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor())\n",
    "    policy.reward_episode= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast length:   999\tAverage length: 19.89\n",
      "Episode 10\tLast length:   999\tAverage length: 113.51\n",
      "Episode 20\tLast length:   999\tAverage length: 198.18\n",
      "Episode 30\tLast length:   999\tAverage length: 274.75\n",
      "Episode 40\tLast length:   999\tAverage length: 344.00\n",
      "Episode 50\tLast length:   999\tAverage length: 406.63\n",
      "Episode 60\tLast length:   999\tAverage length: 463.27\n",
      "Episode 70\tLast length:   999\tAverage length: 514.50\n",
      "Episode 80\tLast length:   999\tAverage length: 560.83\n",
      "Episode 90\tLast length:   999\tAverage length: 602.72\n",
      "Episode 100\tLast length:   999\tAverage length: 640.61\n",
      "Episode 110\tLast length:   999\tAverage length: 674.88\n",
      "Episode 120\tLast length:   999\tAverage length: 705.87\n",
      "Episode 130\tLast length:   999\tAverage length: 733.90\n",
      "Episode 140\tLast length:   999\tAverage length: 759.25\n",
      "Episode 150\tLast length:   999\tAverage length: 782.17\n",
      "Episode 160\tLast length:   999\tAverage length: 802.91\n",
      "Episode 170\tLast length:   999\tAverage length: 821.66\n",
      "Episode 180\tLast length:   999\tAverage length: 838.61\n",
      "Episode 190\tLast length:   999\tAverage length: 853.95\n",
      "Episode 200\tLast length:   999\tAverage length: 867.82\n",
      "Episode 210\tLast length:   999\tAverage length: 880.36\n",
      "Episode 220\tLast length:   999\tAverage length: 891.71\n",
      "Episode 230\tLast length:   999\tAverage length: 901.97\n",
      "Episode 240\tLast length:   999\tAverage length: 911.24\n",
      "Episode 250\tLast length:   999\tAverage length: 919.63\n",
      "Episode 260\tLast length:   999\tAverage length: 927.22\n",
      "Episode 270\tLast length:   999\tAverage length: 934.09\n",
      "Episode 280\tLast length:   999\tAverage length: 940.29\n",
      "Episode 290\tLast length:   999\tAverage length: 945.91\n",
      "Episode 300\tLast length:   999\tAverage length: 950.98\n",
      "Episode 310\tLast length:   999\tAverage length: 955.57\n",
      "Episode 320\tLast length:   999\tAverage length: 959.73\n",
      "Episode 330\tLast length:   999\tAverage length: 963.48\n",
      "Episode 340\tLast length:   999\tAverage length: 966.88\n",
      "Episode 350\tLast length:   999\tAverage length: 969.95\n",
      "Episode 360\tLast length:   999\tAverage length: 972.73\n",
      "Episode 370\tLast length:   999\tAverage length: 975.24\n",
      "Episode 380\tLast length:   999\tAverage length: 977.51\n",
      "Episode 390\tLast length:   999\tAverage length: 979.57\n",
      "Episode 400\tLast length:   999\tAverage length: 981.42\n",
      "Episode 410\tLast length:   999\tAverage length: 983.10\n",
      "Episode 420\tLast length:   999\tAverage length: 984.62\n",
      "Episode 430\tLast length:   999\tAverage length: 986.00\n",
      "Episode 440\tLast length:   999\tAverage length: 987.24\n",
      "Episode 450\tLast length:   999\tAverage length: 988.37\n",
      "Episode 460\tLast length:   999\tAverage length: 989.38\n",
      "Solved! Running reward is now 990.0367117469984 and the last episode runs to 999 time steps!\n"
     ]
    }
   ],
   "source": [
    "game, board, reward, discount = make_game()\n",
    "    \n",
    "policy = Policy(board.layered_board.view(-1).shape[0], 5)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "episodes = 1000\n",
    "main(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
