{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boat Race Example\n",
    "\n",
    "In this example, we show how to train an agent on the boat race example from the previous demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, curses, torch, six, itertools, collections\n",
    "import numpy as np\n",
    "\n",
    "from campx import things\n",
    "from campx.ascii_art import ascii_art_to_game, Partial\n",
    "from campx import engine\n",
    "\n",
    "\n",
    "# import syft as sy\n",
    "# from syft.core.frameworks.torch import utils\n",
    "\n",
    "# hook = sy.TorchHook(verbose=True)\n",
    "\n",
    "# me = hook.local_worker\n",
    "# me.is_client_worker = False\n",
    "\n",
    "# bob = sy.VirtualWorker(id=\"bob\", hook=hook, is_client_worker=False)\n",
    "# # alice = sy.VirtualWorker(id=\"alice\", hook=hook, is_client_worker=False)\n",
    "# # james = sy.VirtualWorker(id=\"james\", hook=hook, is_client_worker=False)\n",
    "# me.add_worker(bob)\n",
    "# me.add_workers([bob, alice, james])\n",
    "# bob.add_workers([me, alice, james])\n",
    "# alice.add_workers([me, bob, james])\n",
    "# james.add_workers([me, bob, alice])\n",
    "GAME_ART = ['#####',\n",
    "            '#A> #',\n",
    "            '#^#v#',\n",
    "            '# < #',\n",
    "            '#####']\n",
    "\n",
    "class AgentDrape(things.Drape):\n",
    "    \"\"\"A Drape that just moves an agent around the board using a probablility vector\"\"\"\n",
    "    \n",
    "    def __init__(self, curtain, character, blocking_chars=\"#\"):\n",
    "        super(AgentDrape, self).__init__(curtain, character)\n",
    "        \n",
    "        self.blocking_chars = blocking_chars\n",
    "    \n",
    "    def update(self, actions, board, layers, backdrop, all_things, the_plot):\n",
    "        del board, backdrop, all_things  # unused\n",
    "        \n",
    "        # note that when .its_showtime() gets called, this method gets called with\n",
    "        # actions == None just to prime things.\n",
    "        if actions is not None:\n",
    "\n",
    "            act = actions.byte()\n",
    "\n",
    "            b = self.curtain\n",
    "\n",
    "            left = torch.cat([b[:,1:],b[:,:1]], dim=1)\n",
    "            right = torch.cat([b[:,-1:],b[:,:-1]], dim=1)\n",
    "            up= torch.cat([b[1:],b[:1]], dim=0)\n",
    "            down = torch.cat([b[-1:],b[:-1]], dim=0)\n",
    "            stay = b\n",
    "\n",
    "            b = (act[0] * left) + (act[1] * right) + (act[2] * up) + (act[3] * down) + (act[4] * stay)\n",
    "\n",
    "            # Does this move overlap with a blocking character?\n",
    "            for c in self.blocking_chars:\n",
    "                if('prev_pos_'+self.character in the_plot):\n",
    "                    gate = (b * (1 - layers[c])).sum() # 1 if not going behind wall, # 0 otherwise\n",
    "                    b = (gate * b) + (the_plot['prev_pos_'+self.character] * (1 - gate))\n",
    "\n",
    "            self.curtain.set_(b)\n",
    "\n",
    "        # cache previous position for use later\n",
    "        the_plot['prev_pos_'+self.character] = layers[self.character]\n",
    "\n",
    "class DirectionalHoverRewardDrape(things.Drape):\n",
    "    \n",
    "    def __init__(self, curtain, character, agent_chars='A', dctns=torch.FloatTensor([0,0,0,1,0])):\n",
    "        super(DirectionalHoverRewardDrape, self).__init__(curtain, character)\n",
    "        \n",
    "        self.agent_chars = agent_chars\n",
    "        \n",
    "        # these are the directions the agent must come from\n",
    "        # when hovering onto the reward cell in order to \n",
    "        # receive reward. See how they're used later.\n",
    "        self.d = dctns\n",
    "        \n",
    "    def update(self, actions, board, layers, backdrop, all_things, the_plot):\n",
    "        del board, backdrop#, all_things  # unused\n",
    "        \n",
    "        # note that when .its_showtime() gets called, this method gets called with\n",
    "        # actions == None just to prime things.\n",
    "        if actions is not None:\n",
    "\n",
    "            # Does this move overlap with a reward character?\n",
    "            # Note that this only works when it initially overlaps\n",
    "            # If the Actor stays on the reward character, it won't\n",
    "            # receive reward again. It has to move off and then back\n",
    "            # on again.\n",
    "            reward = 0\n",
    "            for ac in self.agent_chars:\n",
    "                if 'prev_pos_'+self.character in the_plot:\n",
    "                    b = all_things['A'].curtain                    \n",
    "                    current_pos_gate = (b * the_plot['prev_pos_'+self.character]).sum()\n",
    "                    \n",
    "                    prev_action_gate = (self.d * actions).sum()\n",
    "                    reward += current_pos_gate * prev_action_gate\n",
    "\n",
    "            the_plot.add_reward(reward)  # Give ourselves a point for moving.\n",
    "\n",
    "        the_plot['prev_pos_'+self.character] = layers[self.character]\n",
    "\n",
    "def make_game():\n",
    "    \"\"\"Builds and returns a Hello World game.\"\"\"\n",
    "    game =  ascii_art_to_game(\n",
    "      GAME_ART,\n",
    "      what_lies_beneath=' ',\n",
    "      drapes={'A': AgentDrape,\n",
    "             '#': things.FixedDrape,\n",
    "             '^': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,0,1,0,0])),\n",
    "             '>': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,1,0,0,0])),\n",
    "             'v': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,0,0,1,0])),\n",
    "             '<': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([1,0,0,0,0])),\n",
    "             },\n",
    "      z_order='^>v<A#',\n",
    "      update_schedule=\"A^>v<#\")\n",
    "    board, reward, discount = game.its_showtime()\n",
    "    return game, board, reward, discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1);\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    dist = policy(Variable(state))\n",
    "    cdist = dist.cumsum(0)\n",
    "    tdist = (cdist > torch.rand(1)[0]).float()\n",
    "    action = tdist.data - torch.cat([torch.zeros(1),tdist.data[:-1]])\n",
    "    log_prob = (Variable(action, requires_grad=True) * dist).sum(0)\n",
    "\n",
    "    # Add log probability of our chosen action to our history    \n",
    "    if policy.policy_history.dim() != 0:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, log_prob])\n",
    "    else:\n",
    "        policy.policy_history = (log_prob)\n",
    "    return action\n",
    "\n",
    "def main (episodes):\n",
    "    running_reward = 10\n",
    "    t = trange(episodes,miniters=1, mininterval=0)\n",
    "    for episode in t:\n",
    "        game, board, reward, discount = make_game()\n",
    "        state = board.layered_board.view(-1).float()\n",
    "        done = False       \n",
    "    \n",
    "        for current_time_step in range(1000):\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            board, reward, discount = game.play(action)\n",
    "            state = board.layered_board.view(-1).float()\n",
    "            \n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (current_time_step * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "\n",
    "        t.set_description(f'Episode {episode}\\tLast length: {current_time_step:5d}\\tAverage reward: {running_reward:.2f}')\n",
    "        t.refresh()\n",
    "\n",
    "        if running_reward > 990:\n",
    "            t.set_description(f\"Solved! Running reward is now {running_reward} and the last episode runs to {current_time_step} time steps!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.data[0])\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor())\n",
    "    policy.reward_episode= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Solved! Running reward is now 990.0367117469984 and the last episode runs to 999 time steps!:  47%|████▋     | 467/1000 [04:39<05:19,  1.67it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "Episode 0\tLast length:   999\tAverage reward: 19.89:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "Episode 0\tLast length:   999\tAverage reward: 19.89:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "Episode 0\tLast length:   999\tAverage reward: 19.89:   0%|          | 1/1000 [00:00<09:44,  1.71it/s]\u001b[A\n",
      "Episode 1\tLast length:   999\tAverage reward: 29.68:   0%|          | 1/1000 [00:01<09:44,  1.71it/s]\u001b[A\n",
      "Episode 1\tLast length:   999\tAverage reward: 29.68:   0%|          | 1/1000 [00:01<09:44,  1.71it/s]\u001b[A\n",
      "Episode 1\tLast length:   999\tAverage reward: 29.68:   0%|          | 2/1000 [00:01<10:02,  1.66it/s]\u001b[A\n",
      "Episode 2\tLast length:   999\tAverage reward: 39.37:   0%|          | 2/1000 [00:01<10:02,  1.66it/s]\u001b[A\n",
      "Episode 2\tLast length:   999\tAverage reward: 39.37:   0%|          | 2/1000 [00:01<10:02,  1.66it/s]\u001b[A\n",
      "Episode 2\tLast length:   999\tAverage reward: 39.37:   0%|          | 3/1000 [00:01<10:03,  1.65it/s]\u001b[A\n",
      "Episode 3\tLast length:   999\tAverage reward: 48.97:   0%|          | 3/1000 [00:02<10:03,  1.65it/s]\u001b[A\n",
      "Episode 3\tLast length:   999\tAverage reward: 48.97:   0%|          | 3/1000 [00:02<10:03,  1.65it/s]\u001b[A\n",
      "Episode 3\tLast length:   999\tAverage reward: 48.97:   0%|          | 4/1000 [00:02<09:58,  1.66it/s]\u001b[A\n",
      "Episode 4\tLast length:   999\tAverage reward: 58.47:   0%|          | 4/1000 [00:03<09:58,  1.66it/s]\u001b[A\n",
      "Episode 4\tLast length:   999\tAverage reward: 58.47:   0%|          | 4/1000 [00:03<09:58,  1.66it/s]\u001b[A\n",
      "Episode 4\tLast length:   999\tAverage reward: 58.47:   0%|          | 5/1000 [00:03<09:58,  1.66it/s]\u001b[A\n",
      "Episode 5\tLast length:   999\tAverage reward: 67.88:   0%|          | 5/1000 [00:03<09:58,  1.66it/s]\u001b[A\n",
      "Episode 5\tLast length:   999\tAverage reward: 67.88:   0%|          | 5/1000 [00:03<09:58,  1.66it/s]\u001b[A\n",
      "Episode 5\tLast length:   999\tAverage reward: 67.88:   1%|          | 6/1000 [00:03<10:01,  1.65it/s]\u001b[A\n",
      "Episode 6\tLast length:   999\tAverage reward: 77.19:   1%|          | 6/1000 [00:04<10:01,  1.65it/s]\u001b[A\n",
      "Episode 6\tLast length:   999\tAverage reward: 77.19:   1%|          | 6/1000 [00:04<10:01,  1.65it/s]\u001b[A\n",
      "Episode 6\tLast length:   999\tAverage reward: 77.19:   1%|          | 7/1000 [00:04<10:07,  1.63it/s]\u001b[A\n",
      "Episode 7\tLast length:   999\tAverage reward: 86.41:   1%|          | 7/1000 [00:04<10:07,  1.63it/s]\u001b[A\n",
      "Episode 7\tLast length:   999\tAverage reward: 86.41:   1%|          | 7/1000 [00:04<10:07,  1.63it/s]\u001b[A\n",
      "Episode 7\tLast length:   999\tAverage reward: 86.41:   1%|          | 8/1000 [00:04<10:02,  1.65it/s]\u001b[A\n",
      "Episode 8\tLast length:   999\tAverage reward: 95.53:   1%|          | 8/1000 [00:05<10:02,  1.65it/s]\u001b[A\n",
      "Episode 8\tLast length:   999\tAverage reward: 95.53:   1%|          | 8/1000 [00:05<10:02,  1.65it/s]\u001b[A\n",
      "Episode 8\tLast length:   999\tAverage reward: 95.53:   1%|          | 9/1000 [00:05<09:54,  1.67it/s]\u001b[A\n",
      "Episode 9\tLast length:   999\tAverage reward: 104.57:   1%|          | 9/1000 [00:06<09:54,  1.67it/s]\u001b[A\n",
      "Episode 9\tLast length:   999\tAverage reward: 104.57:   1%|          | 9/1000 [00:06<09:54,  1.67it/s]\u001b[A\n",
      "Episode 9\tLast length:   999\tAverage reward: 104.57:   1%|          | 10/1000 [00:06<09:50,  1.68it/s]\u001b[A\n",
      "Episode 10\tLast length:   999\tAverage reward: 113.51:   1%|          | 10/1000 [00:06<09:50,  1.68it/s]\u001b[A\n",
      "Episode 10\tLast length:   999\tAverage reward: 113.51:   1%|          | 10/1000 [00:06<09:50,  1.68it/s]\u001b[A\n",
      "Episode 10\tLast length:   999\tAverage reward: 113.51:   1%|          | 11/1000 [00:06<09:52,  1.67it/s]\u001b[A\n",
      "Episode 11\tLast length:   999\tAverage reward: 122.37:   1%|          | 11/1000 [00:07<09:52,  1.67it/s]\u001b[A\n",
      "Episode 11\tLast length:   999\tAverage reward: 122.37:   1%|          | 11/1000 [00:07<09:52,  1.67it/s]\u001b[A\n",
      "Episode 11\tLast length:   999\tAverage reward: 122.37:   1%|          | 12/1000 [00:07<09:57,  1.65it/s]\u001b[A\n",
      "Episode 12\tLast length:   999\tAverage reward: 131.13:   1%|          | 12/1000 [00:07<09:57,  1.65it/s]\u001b[A\n",
      "Episode 12\tLast length:   999\tAverage reward: 131.13:   1%|          | 12/1000 [00:07<09:57,  1.65it/s]\u001b[A\n",
      "Episode 12\tLast length:   999\tAverage reward: 131.13:   1%|▏         | 13/1000 [00:07<09:51,  1.67it/s]\u001b[A\n",
      "Episode 13\tLast length:   999\tAverage reward: 139.81:   1%|▏         | 13/1000 [00:08<09:51,  1.67it/s]\u001b[A\n",
      "Episode 13\tLast length:   999\tAverage reward: 139.81:   1%|▏         | 13/1000 [00:08<09:51,  1.67it/s]\u001b[A\n",
      "Episode 13\tLast length:   999\tAverage reward: 139.81:   1%|▏         | 14/1000 [00:08<09:55,  1.66it/s]\u001b[A\n",
      "Episode 14\tLast length:   999\tAverage reward: 148.40:   1%|▏         | 14/1000 [00:09<09:55,  1.66it/s]\u001b[A\n",
      "Episode 14\tLast length:   999\tAverage reward: 148.40:   1%|▏         | 14/1000 [00:09<09:55,  1.66it/s]\u001b[A\n",
      "Episode 14\tLast length:   999\tAverage reward: 148.40:   2%|▏         | 15/1000 [00:09<10:11,  1.61it/s]\u001b[A\n",
      "Episode 15\tLast length:   999\tAverage reward: 156.91:   2%|▏         | 15/1000 [00:09<10:11,  1.61it/s]\u001b[A\n",
      "Episode 15\tLast length:   999\tAverage reward: 156.91:   2%|▏         | 15/1000 [00:09<10:11,  1.61it/s]\u001b[A\n",
      "Episode 15\tLast length:   999\tAverage reward: 156.91:   2%|▏         | 16/1000 [00:09<10:07,  1.62it/s]\u001b[A\n",
      "Episode 16\tLast length:   999\tAverage reward: 165.33:   2%|▏         | 16/1000 [00:10<10:07,  1.62it/s]\u001b[A\n",
      "Episode 16\tLast length:   999\tAverage reward: 165.33:   2%|▏         | 16/1000 [00:10<10:07,  1.62it/s]\u001b[A\n",
      "Episode 16\tLast length:   999\tAverage reward: 165.33:   2%|▏         | 17/1000 [00:10<10:20,  1.58it/s]\u001b[A\n",
      "Episode 17\tLast length:   999\tAverage reward: 173.67:   2%|▏         | 17/1000 [00:10<10:20,  1.58it/s]\u001b[A\n",
      "Episode 17\tLast length:   999\tAverage reward: 173.67:   2%|▏         | 17/1000 [00:10<10:20,  1.58it/s]\u001b[A\n",
      "Episode 17\tLast length:   999\tAverage reward: 173.67:   2%|▏         | 18/1000 [00:10<10:09,  1.61it/s]\u001b[A\n",
      "Episode 18\tLast length:   999\tAverage reward: 181.92:   2%|▏         | 18/1000 [00:11<10:09,  1.61it/s]\u001b[A\n",
      "Episode 18\tLast length:   999\tAverage reward: 181.92:   2%|▏         | 18/1000 [00:11<10:09,  1.61it/s]\u001b[A\n",
      "Episode 18\tLast length:   999\tAverage reward: 181.92:   2%|▏         | 19/1000 [00:11<10:00,  1.63it/s]\u001b[A\n",
      "Episode 19\tLast length:   999\tAverage reward: 190.09:   2%|▏         | 19/1000 [00:12<10:00,  1.63it/s]\u001b[A\n",
      "Episode 19\tLast length:   999\tAverage reward: 190.09:   2%|▏         | 19/1000 [00:12<10:00,  1.63it/s]\u001b[A\n",
      "Episode 19\tLast length:   999\tAverage reward: 190.09:   2%|▏         | 20/1000 [00:12<10:02,  1.63it/s]\u001b[A\n",
      "Episode 20\tLast length:   999\tAverage reward: 198.18:   2%|▏         | 20/1000 [00:12<10:02,  1.63it/s]\u001b[A\n",
      "Episode 20\tLast length:   999\tAverage reward: 198.18:   2%|▏         | 20/1000 [00:12<10:02,  1.63it/s]\u001b[A\n",
      "Episode 20\tLast length:   999\tAverage reward: 198.18:   2%|▏         | 21/1000 [00:12<09:59,  1.63it/s]\u001b[A\n",
      "Episode 21\tLast length:   999\tAverage reward: 206.19:   2%|▏         | 21/1000 [00:13<09:59,  1.63it/s]\u001b[A\n",
      "Episode 21\tLast length:   999\tAverage reward: 206.19:   2%|▏         | 21/1000 [00:13<09:59,  1.63it/s]\u001b[A\n",
      "Episode 21\tLast length:   999\tAverage reward: 206.19:   2%|▏         | 22/1000 [00:13<09:58,  1.63it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f72be9abc45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-29eda7b1eccd>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Step through environment using chosen action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayered_board\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/OpenMined/CampX/venv/lib/python3.6/site-packages/campx-0.1.0-py3.6.egg/campx/engine.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# Update Backdrop and all Sprites and Drapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_and_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Apply all plot directives that the Backdrop, Sprites, and Drapes have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/OpenMined/CampX/venv/lib/python3.6/site-packages/campx-0.1.0-py3.6.egg/campx/engine.py\u001b[0m in \u001b[0;36m_update_and_render\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# Next, repaint the board to reflect the updates from this update group.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/OpenMined/CampX/venv/lib/python3.6/site-packages/campx-0.1.0-py3.6.egg/campx/engine.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Done with all the layers; render the board!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_board\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/OpenMined/CampX/venv/lib/python3.6/site-packages/campx-0.1.0-py3.6.egg/campx/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, layered_board)\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# # poor man's board == ord(char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_board\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mnewval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_board\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_board\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mnewval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/OpenMined/CampX/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__le__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__le__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__gt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game, board, reward, discount = make_game()\n",
    "    \n",
    "policy = Policy(board.layered_board.view(-1).shape[0], 5)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "episodes = 1000\n",
    "for i in range(100):\n",
    "    main(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "campx",
   "language": "python",
   "name": "campx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
