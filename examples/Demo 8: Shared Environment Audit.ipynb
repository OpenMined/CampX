{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Environment Execution Example (Boat Race)\n",
    "\n",
    "In this example, we show how to run an environment (that is created on this machine) on a remote worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker james already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "import os, sys, curses, torch, six, itertools, collections\n",
    "import numpy as np\n",
    "\n",
    "from campx import things\n",
    "from campx.ascii_art import ascii_art_to_game, Partial\n",
    "from campx import engine\n",
    "\n",
    "\n",
    "import syft as sy\n",
    "from syft.core.frameworks.torch import utils\n",
    "\n",
    "hook = sy.TorchHook(verbose=True)\n",
    "\n",
    "me = hook.local_worker\n",
    "me.is_client_worker = False\n",
    "\n",
    "bob = sy.VirtualWorker(id=\"bob\", hook=hook, is_client_worker=False)\n",
    "alice = sy.VirtualWorker(id=\"alice\", hook=hook, is_client_worker=False)\n",
    "james = sy.VirtualWorker(id=\"james\", hook=hook, is_client_worker=False)\n",
    "me.add_worker(bob)\n",
    "me.add_workers([bob, alice, james])\n",
    "bob.add_workers([me, alice, james])\n",
    "alice.add_workers([me, bob, james])\n",
    "james.add_workers([me, bob, alice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_ART = ['#####',\n",
    "            '#A> #',\n",
    "            '#^#v#',\n",
    "            '# < #',\n",
    "            '#####']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDrape(things.Drape):\n",
    "    \"\"\"A Drape that just moves an agent around the board using a probablility vector\"\"\"\n",
    "    \n",
    "    def __init__(self, curtain, character, blocking_chars=\"#\"):\n",
    "        super(AgentDrape, self).__init__(curtain, character)\n",
    "        \n",
    "        self.blocking_chars = blocking_chars\n",
    "    \n",
    "    def update(self, actions, board, layers, backdrop, all_things, the_plot):\n",
    "\n",
    "        del board, backdrop, all_things  # unused\n",
    "        \n",
    "        # note that when .its_showtime() gets called, this method gets called with\n",
    "        # actions == None just to prime things.\n",
    "        if actions is not None:\n",
    "\n",
    "            act = actions#.byte()\n",
    "\n",
    "            b = self.curtain\n",
    "\n",
    "            if(not isinstance(b, torch.LongTensor)):\n",
    "                b = b.long()\n",
    "            \n",
    "            left = torch.cat([b[:,1:],b[:,:1]], dim=1)\n",
    "            right = torch.cat([b[:,-1:],b[:,:-1]], dim=1)\n",
    "            up= torch.cat([b[1:],b[:1]], dim=0)\n",
    "            down = torch.cat([b[-1:],b[:-1]], dim=0)\n",
    "            stay = b\n",
    "            \n",
    "            \n",
    "            # automatic broadcasting doesn't work for MPC at the moment\n",
    "            # so we need to expand tensors manually\n",
    "            left_shape = list(left.get_shape())\n",
    "            n_elems_in_left = torch.IntTensor(left_shape).prod()\n",
    "            act_left = act[0:1].expand(n_elems_in_left).contiguous().view(left_shape)            \n",
    "            act_right = act[1:2].expand(n_elems_in_left).contiguous().view(left_shape)            \n",
    "            act_up = act[2:3].expand(n_elems_in_left).contiguous().view(left_shape)                        \n",
    "            act_down = act[3:4].expand(n_elems_in_left).contiguous().view(left_shape)                        \n",
    "            act_stay = act[4:].expand(n_elems_in_left).contiguous().view(left_shape)                        \n",
    "\n",
    "            b = (act_left * left) + \\\n",
    "            (act_right * right) + \\\n",
    "            (act_up * up) + \\\n",
    "            (act_down * down) + \\\n",
    "            (act_stay * stay)\n",
    "            \n",
    "            # Does this move overlap with a blocking character?\n",
    "            for c in self.blocking_chars:\n",
    "                if('prev_pos_'+self.character in the_plot):\n",
    "                    if(not isinstance(layers[c], torch.LongTensor)):\n",
    "                        layers[c] = layers[c].long()\n",
    "                    ones = (layers[c] * 0)\n",
    "                    ones = (ones >= ones) * (ones <= ones)\n",
    "                    if(not isinstance(ones, torch.LongTensor)):\n",
    "                        ones = ones.long()\n",
    "                    diff = (ones - layers[c])\n",
    "                    mul = (b * diff)\n",
    "    \n",
    "                    gate = mul[0] + mul[1] + mul[2] + mul[3] + mul[4] # 1 if not going behind wall, # 0 otherwise\n",
    "                    gate = gate.sum(0)\n",
    "\n",
    "                    gate = gate.expand(n_elems_in_left).contiguous().view(left_shape)\n",
    "\n",
    "                    oneminusgate = (ones - gate)\n",
    "\n",
    "                    gate_times_b = (gate * b)\n",
    "                    \n",
    "                    if(not isinstance(the_plot['prev_pos_'+self.character], torch.LongTensor)):\n",
    "                        the_plot['prev_pos_'+self.character] = the_plot['prev_pos_'+self.character].long()\n",
    "                        \n",
    "                    plot_times_oneminusgate = (the_plot['prev_pos_'+self.character] * oneminusgate)\n",
    "\n",
    "                    b = gate_times_b + plot_times_oneminusgate\n",
    "            \n",
    "            #Â changed from .set_() because for MPC it doesn't seem to work yet\n",
    "            if(isinstance(self.curtain.child, sy._SNNTensor)):\n",
    "                self.curtain.child.child *= 0\n",
    "                self.curtain.child.child += b.child.child\n",
    "            else:\n",
    "                if(not isinstance(self.curtain, torch.LongTensor)):\n",
    "\n",
    "                    self.curtain.set_(b.byte())\n",
    "                else:\n",
    "                    self.curtain.set_(b)                    \n",
    "\n",
    "        # cache previous position for use later\n",
    "        the_plot['prev_pos_'+self.character] = layers[self.character]\n",
    "\n",
    "class DirectionalHoverRewardDrape(things.Drape):\n",
    "    \n",
    "    def __init__(self, curtain, character, agent_chars='A', dctns=torch.FloatTensor([0,0,0,1,0])):\n",
    "        super(DirectionalHoverRewardDrape, self).__init__(curtain, character)\n",
    "        \n",
    "        self.agent_chars = agent_chars\n",
    "        \n",
    "        # these are the directions the agent must come from\n",
    "        # when hovering onto the reward cell in order to \n",
    "        # receive reward. See how they're used later.\n",
    "        self.d = dctns\n",
    "        \n",
    "    def update(self, actions, board, layers, backdrop, all_things, the_plot):\n",
    "\n",
    "        del board, backdrop#, all_things  # unused\n",
    "        \n",
    "        # note that when .its_showtime() gets called, this method gets called with\n",
    "        # actions == None just to prime things.\n",
    "        if actions is not None:\n",
    "\n",
    "            # Does this move overlap with a reward character?\n",
    "            # Note that this only works when it initially overlaps\n",
    "            # If the Actor stays on the reward character, it won't\n",
    "            # receive reward again. It has to move off and then back\n",
    "            # on again.\n",
    "            reward = 0\n",
    "            for ac in self.agent_chars:\n",
    "                if 'prev_pos_'+self.character in the_plot:\n",
    "                    \n",
    "                    b = all_things['A'].curtain      \n",
    "                    \n",
    "                    current_pos_gate = (b * the_plot['prev_pos_'+self.character]).sum()\n",
    "                    \n",
    "                    if(not isinstance(self.d, torch.LongTensor)):\n",
    "                        self.d = self.d.long()\n",
    "                        \n",
    "                    prev_action_gate = (self.d * actions).sum()\n",
    "                    reward = reward + (current_pos_gate * prev_action_gate)\n",
    "\n",
    "\n",
    "            the_plot.add_reward(reward)  # Accumulate reward (which might be zero)\n",
    "\n",
    "\n",
    "        the_plot['prev_pos_'+self.character] = layers[self.character]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_game():\n",
    "    \"\"\"Builds and returns a Hello World game.\"\"\"\n",
    "    game =  ascii_art_to_game(\n",
    "      GAME_ART,\n",
    "      what_lies_beneath=' ',\n",
    "      drapes={'A': AgentDrape,\n",
    "             '#': things.FixedDrape,\n",
    "             '^': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,0,1,0,0])),\n",
    "             '>': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,1,0,0,0])),\n",
    "             'v': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([0,0,0,1,0])),\n",
    "             '<': Partial(DirectionalHoverRewardDrape, dctns=torch.FloatTensor([1,0,0,0,0])),\n",
    "             },\n",
    "      z_order='^>v<A#',\n",
    "      update_schedule=\"A^>v<#\")\n",
    "    board, reward, discount = game.its_showtime()\n",
    "    return game, board, reward, discount\n",
    "\n",
    "game, board, reward, discount = make_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  35   35   35   35   35\n",
       "  35   65   62   32   35\n",
       "  35   94   35  118   35\n",
       "  35   32   60   32   35\n",
       "  35   35   35   35   35\n",
       "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1);\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, self.action_space, bias=False)\n",
    "#         self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):    \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    dist = policy(Variable(state))\n",
    "    cdist = dist.cumsum(0)\n",
    "    tdist = (cdist > torch.rand(1)[0]).float()\n",
    "    action = tdist.data - torch.cat([torch.zeros(1),tdist.data[:-1]])\n",
    "    log_prob = (Variable(action, requires_grad=True) * dist).sum(0)\n",
    "\n",
    "    # Add log probability of our chosen action to our history    \n",
    "    if policy.policy_history.dim() != 0:\n",
    "        policy.policy_history = torch.cat([policy.policy_history, log_prob])\n",
    "    else:\n",
    "        policy.policy_history = (log_prob)\n",
    "    return action\n",
    "\n",
    "def main (episodes):\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        running_reward = 10\n",
    "        game, board, reward, discount = make_game()\n",
    "        state = board.layered_board.view(-1).float()\n",
    "        done = False       \n",
    "        \n",
    "        for time in range(200):\n",
    "\n",
    "            action = select_action(state)\n",
    "            # Step through environment using chosen action\n",
    "            board, reward, discount = game.play(action.long())\n",
    "            state = board.layered_board.view(-1).float()\n",
    "            \n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "\n",
    "        update_policy()\n",
    "\n",
    "        print('Episode {}\\tAverage reward: {:.2f}'.format(episode, running_reward))\n",
    "\n",
    "        if running_reward > 990:\n",
    "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.data[0])\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor())\n",
    "    policy.reward_episode= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, board, reward, discount = make_game()\n",
    "    \n",
    "policy = Policy(board.layered_board.view(-1).shape[0], 5)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit the agent and collect reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0  0  1  0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1x5]\n",
      "\n",
      "\n",
      "  35   35   35   35   35\n",
      "  35   32   62   32   35\n",
      "  35   65   35  118   35\n",
      "  35   32   60   32   35\n",
      "  35   35   35   35   35\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]\n",
      "\n",
      "\n",
      " 0  0  0  1  0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1x5]\n",
      "\n",
      "\n",
      "  35   35   35   35   35\n",
      "  35   32   62   32   35\n",
      "  35   94   35  118   35\n",
      "  35   65   60   32   35\n",
      "  35   35   35   35   35\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]\n",
      "\n",
      "\n",
      " 0  0  0  1  0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1x5]\n",
      "\n",
      "\n",
      "  35   35   35   35   35\n",
      "  35   32   62   32   35\n",
      "  35   65   35  118   35\n",
      "  35   32   60   32   35\n",
      "  35   35   35   35   35\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]\n",
      "\n",
      "\n",
      " 0  0  0  1  0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1x5]\n",
      "\n",
      "\n",
      "  35   35   35   35   35\n",
      "  35   32   62   32   35\n",
      "  35   94   35  118   35\n",
      "  35   65   60   32   35\n",
      "  35   35   35   35   35\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "W = policy.l1.weight.data\n",
    "\n",
    "W = W.fix_precision().share(bob,alice)\n",
    "game, board, reward, discount = make_game()\n",
    "game.share(bob, alice)\n",
    "\n",
    "state = board.layered_board.view(-1)\n",
    "\n",
    "rewards = list()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    pred = W.mm(state.view(-1,1)).wrap(True).view(1,-1)\n",
    "    action = pred.argmax()\n",
    "\n",
    "    # action from fixed-precision -> long\n",
    "    out = action.child.truncate(action.child.child, action.child)[0]\n",
    "    print((out+0).get())\n",
    "    board, reward, discount = game.play(out.view(-1))\n",
    "    state = board.layered_board.view(-1)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    print((board.board+0).get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  35   35   35   35   35\n",
       "  35   32   62   65   35\n",
       "  35   94   35  118   35\n",
       "  35   32   60   32   35\n",
       "  35   35   35   35   35\n",
       "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will move the 65 around the board ([left, right, up, down, stay])\n",
    "# run it multiple times. Notice how the \"65\" is blocked by all \"35\" items\n",
    "\n",
    "act = torch.FloatTensor([0,1,0,0,0]).long().share(bob,alice)\n",
    "board, reward, discout = game.play(act)\n",
    "\n",
    "print((reward + 0).get())\n",
    "\n",
    "b = (board.board + 0).get()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  35   35   35   35   35\n",
       "  35   32   62   32   35\n",
       "  35   94   35  118   35\n",
       "  35   32   60   65   35\n",
       "  35   35   35   35   35\n",
       "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will move the 65 around the board ([left, right, up, down, stay])\n",
    "# run it multiple times. Notice how the \"65\" is blocked by all \"35\" items\n",
    "\n",
    "act = torch.FloatTensor([0,0,0,1,0]).long().share(bob,alice)\n",
    "board, reward, discout = game.play(act)\n",
    "\n",
    "print((reward+0).get())\n",
    "\n",
    "(board.board * 1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  35   35   35   35   35\n",
       "  35   32   62   32   35\n",
       "  35   94   35  118   35\n",
       "  35   65   60   32   35\n",
       "  35   35   35   35   35\n",
       "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will move the 65 around the board ([left, right, up, down, stay])\n",
    "# run it multiple times. Notice how the \"65\" is blocked by all \"35\" items\n",
    "\n",
    "act = torch.FloatTensor([1,0,0,0,0]).long().share(bob, alice)\n",
    "board, reward, discout = game.play(act)\n",
    "\n",
    "print((reward+0).get())\n",
    "\n",
    "(board.board * 1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      "[syft.core.frameworks.torch.tensor.LongTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  35   35   35   35   35\n",
       "  35   65   62   32   35\n",
       "  35   94   35  118   35\n",
       "  35   32   60   32   35\n",
       "  35   35   35   35   35\n",
       "[syft.core.frameworks.torch.tensor.LongTensor of size 5x5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will move the 65 around the board ([left, right, up, down, stay])\n",
    "# run it multiple times. Notice how the \"65\" is blocked by all \"35\" items\n",
    "\n",
    "act = torch.FloatTensor([0,0,1,0,0]).long().share(bob, alice)\n",
    "board, reward, discout = game.play(act)\n",
    "\n",
    "print((reward+0).get())\n",
    "\n",
    "(board.board * 1).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
